{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2f9a0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading and preprocessing\n",
    "corpusFile=open('Corpus.txt','r', encoding='utf-8')\n",
    "corpus = corpusFile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8901cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts to lowercase\n",
    "corpus = corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e64f5987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts to list of sentences\n",
    "import nltk\n",
    "sent_tokens = nltk.sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6afd65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# lemmatize the tokens\n",
    "def Lemmatize(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65d9d25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation dictionary\n",
    "##! The ord() function returns the number representing\n",
    "##! the unicode code of a specified character.\n",
    "\n",
    "##! The dict() function creates a dictionary.\n",
    "##! keyword arguments as much as you like, separated by comma: \n",
    "##! key = value, key = value\n",
    "import string\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "563a6159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words and punctuations\n",
    "def normalize(text):\n",
    "    stopEnglish = set(stopwords.words('english'))\n",
    "    return Lemmatize([token for token in nltk.word_tokenize(text.lower().translate(remove_punct_dict)) if token not in stopEnglish])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e65e003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle of greetings\n",
    "\n",
    "import random\n",
    "##! The choice() method returns a randomly selected element\n",
    "##! from the specified sequence.\n",
    "\n",
    "def greet(sentence):\n",
    "    GREETING_INPUTS = (\"welcome\",\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "    GREETING_RESPONSES = [\"Welcome\",\"Hi\", \"Hey\", \"*nods*\", \"Hi there\", \"Hello\", \"I am glad! You are talking to me\"] \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "07894297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import csv\n",
    "import _pickle as cPickle\n",
    "import re\n",
    "\n",
    "class EmotionAnalyser:\n",
    "    __instance = None\n",
    "\n",
    "    @staticmethod\n",
    "    def getInstance(base_path):\n",
    "        \"\"\" Static access method. \"\"\"\n",
    "        if EmotionAnalyser.__instance == None:\n",
    "            EmotionAnalyser(base_path)\n",
    "        return EmotionAnalyser.__instance\n",
    "\n",
    "    def __init__(self,base_path):\n",
    "        if EmotionAnalyser.__instance != None:\n",
    "            raise Exception(\"This class is a singleton!\")\n",
    "        else:\n",
    "            EmotionAnalyser.__instance = self\n",
    "        self.base_path = base_path\n",
    "        self.stoplist = set(stopwords.words(\"english\"))\n",
    "        self.punctuation = ['.',',','\\'','\\\"',':',';','...','-','–','—','(',')','[',']','«','»']\n",
    "        ## read more about https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual\n",
    "        self.tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True) \n",
    "        # lemmatization\n",
    "        self.lemmer = nltk.stem.WordNetLemmatizer()\n",
    "        pass\n",
    "    \n",
    "    def extract_features(self,statement):\n",
    "        statement = statement.lower()\n",
    "        # remove all digits\n",
    "        statement = re.sub(r'[0-9]', ' ', statement)\n",
    "        # remove all (..)\n",
    "        statement = re.sub(r'\\.{2,}', ' ', statement)\n",
    "        # remove all handles\n",
    "        statement = re.sub(r'@.+ ', ' ', statement)        \n",
    "        # tokenization, stopwords and punctuation removal\n",
    "        word_list = [ word for word in self.tokenizer.tokenize(statement) if word not in self.stoplist and word not in string.punctuation]\n",
    "        # lemmatize the tokens\n",
    "        word_list = [self.lemmer.lemmatize(token) for token in word_list]\n",
    "        # one hot encoding\n",
    "        return dict([(word,True) for word in word_list])\n",
    "        # ngrams\n",
    "#         ngram_tubles = ngrams(word_list, 3)\n",
    "#         return dict([(gram,True) for gram in ngram_tubles])\n",
    "#         return dict([(gram,True) for gram in ngram_vocab])\n",
    "\n",
    "    def train(self):        \n",
    "        # load train data from csv file\n",
    "        csv_file = open(self.base_path+'/text_emotion.csv')\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        trainDataset = {}\n",
    "\n",
    "        for index, row in enumerate(csv_reader):\n",
    "            if index != 0:\n",
    "                if row[0] not in trainDataset:\n",
    "                    trainDataset[row[0]] = []\n",
    "                    trainDataset[row[0]].append(row[1])\n",
    "                else :\n",
    "                    trainDataset[row[0]].append(row[1])\n",
    "\n",
    "        # separate train data set into classes of emotion\n",
    "        # Split the dataset into training and testing datasets (80/20)\n",
    "        # build the train features \n",
    "        # build the test features  \n",
    "    \n",
    "        features = {}\n",
    "        thresholds = {}        \n",
    "        spilt_factor = 0.9\n",
    "        features_train = []\n",
    "        features_test = []       \n",
    "\n",
    "        for emotion in trainDataset:\n",
    "            features[emotion] = [(self.extract_features(statement), emotion) for statement in trainDataset[emotion]]\n",
    "            thresholds[emotion] = int(spilt_factor * len(features[emotion]))       \n",
    "            features_train.extend( features[emotion][:thresholds[emotion]] )\n",
    "            features_test.extend( features[emotion][thresholds[emotion]:] )\n",
    "        \n",
    "        if __name__ == \"__main__\":\n",
    "            print (\"sentiments : \",features.keys())\n",
    "            print (\"Number of training records:\", len(features_train))\n",
    "            print (\"Number of test records:\", len(features_test))\n",
    "\n",
    "        # joblib.dump(features_train, \"classifier.sav\")\n",
    "\n",
    "        # use a Naive Bayes classifier and train it\n",
    "        classifier = NaiveBayesClassifier.train(features_train)\n",
    "\n",
    "        if __name__ == \"__main__\":\n",
    "            print (\"Accuracy of the classifier:\", nltk.classify.util.accuracy(classifier, features_test))\n",
    "\n",
    "            informative = classifier.most_informative_features(1000)\n",
    "            print(informative)\n",
    "\n",
    "        # # dump classifier into a file\n",
    "        f = open(self.base_path+'/classifier.pickle', 'wb')        \n",
    "        cPickle.dump(classifier, f)\n",
    "        f.close()\n",
    "        # joblib.dump(classifier, \"classifier.save\")\n",
    " \n",
    "\n",
    "    def classify(self,statement,classifier =None): \n",
    "        if classifier == None :\n",
    "            f = open(self.base_path+'/classifier.pickle', 'rb')  \n",
    "            classifier = cPickle.load(f)\n",
    "            f.close()\n",
    "            # using joblib\n",
    "            # classifier = joblib.load(\"classifier.save\")\n",
    "        probdist = classifier.prob_classify(self.extract_features(statement))\n",
    "        predected_sentiment = probdist.max()\n",
    "        probability = round(probdist.prob(predected_sentiment), 2)\n",
    "        return predected_sentiment, probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2d147b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a', 'foo', 'bar', 'sentences')\n",
      "('is', 'a', 'foo', 'bar', 'sentences', 'and')\n",
      "('a', 'foo', 'bar', 'sentences', 'and', 'i')\n",
      "('foo', 'bar', 'sentences', 'and', 'i', 'want')\n",
      "('bar', 'sentences', 'and', 'i', 'want', 'to')\n",
      "('sentences', 'and', 'i', 'want', 'to', 'ngramize')\n",
      "('and', 'i', 'want', 'to', 'ngramize', 'it')\n"
     ]
    }
   ],
   "source": [
    "# example of bag of n-grams\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "\n",
    "n = 6\n",
    "grams = ngrams(sentence.split(), n)\n",
    "\n",
    "for gram in grams:\n",
    "    print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1be85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "21e3bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiation\n",
    "analyser = EmotionAnalyser.getInstance(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9756d0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiments :  dict_keys(['empty', 'sadness', 'enthusiasm', 'neutral', 'worry', 'surprise', 'love', 'fun', 'hate', 'happiness', 'boredom', 'relief', 'anger'])\n",
      "Number of training records: 35996\n",
      "Number of test records: 4004\n",
      "Accuracy of the classifier: 0.17607392607392608\n",
      "[('hate', True), ('bored', True), ('california', True), ('gah', True), ('mother', True), ('as', True), ('boring', True), ('edited', True), ('tube', True), ('finishing', True), ('annoying', True), ('#thingsmummysaid', True), ('accomplish', True), ('ankle', True), ('bein', True), ('blocked', True), ('blond', True), ('bullshit', True), ('chest', True), ('cracked', True), ('cycle', True), ('difficult', True), ('dye', True), ('eee', True), ('fault', True), ('fishing', True), ('font', True), ('goood', True), ('grabbing', True), ('habit', True), ('helpful', True), ('idiot', True), ('insane', True), ('legit', True), ('lemon', True), ('ligament', True), ('mental', True), ('mgmt', True), ('migraine', True), ('rented', True), ('sake', True), ('sausage', True), ('sheesh', True), ('snack', True), ('stomach', True), ('stressing', True), ('taste', True), ('toaster', True), ('wendys', True), (\"where's\", True), ('wicked', True), ('aaa', True), ('beginning', True), ('blend', True), ('brown', True), ('downloading', True), ('ea', True), ('father', True), ('figured', True), ('flag', True), ('gt', True), ('magnet', True), ('mummy', True), ('prematurely', True), ('producer', True), ('pusher', True), ('randomly', True), ('re-dyed', True), ('shout', True), ('teasing', True), ('triple', True), ('windy', True), ('wtf', True), ('stupid', True), ('hell', True), ('fuck', True), ('#fail', True), ('alarm', True), ('argh', True), ('arlington', True), ('author', True), ('backup', True), ('bea', True), ('bff', True), ('bothered', True), ('brake', True), ('bugging', True), ('bunny', True), ('chemistry', True), ('damnit', True), ('delayed', True), ('distracted', True), ('dragged', True), ('error', True), ('foundation', True), ('gorgeous', True), ('happen', True), ('hitting', True), ('hung', True), ('lab', True), ('liking', True), ('lunchhh', True), ('magic', True), ('maintenance', True), ('mikey', True), ('nauseous', True), ('order', True), ('purchase', True), ('rc', True), ('remote', True), ('scheduled', True), ('sittin', True), ('skool', True), ('sneak', True), ('sox', True), ('steve', True), ('talkin', True), ('terrible', True), ('whats', True), ('wifi', True), ('yeh', True), ('d:', True), ('laughing', True), ('realy', True), (\"mother's\", True), ('suck', True), ('aaand', True), ('ach', True), ('asian', True), ('ben', True), ('bf', True), ('boiling', True), ('bonfire', True), ('brace', True), ('common', True), ('counting', True), ('deadline', True), ('detail', True), ('disgusting', True), ('dislike', True), ('documentation', True), ('fashioned', True), ('folio', True), ('framework', True), ('hols', True), ('interested', True), ('journey', True), ('lane', True), ('lawn', True), ('loud', True), ('mentioned', True), ('metaverse', True), ('nj', True), ('nomatter', True), ('nxt', True), ('oneself', True), ('ovi', True), ('particularly', True), ('patience', True), ('pooping', True), ('processing', True), ('quick', True), ('rental', True), ('setting', True), ('shipment', True), ('stream', True), ('tarmac', True), ('tax', True), ('teller', True), ('thier', True), ('toll', True), ('twenty', True), ('ultra', True), ('um', True), ('unpleasant', True), ('wknd', True), ('headache', True), ('age', True), ('brb', True), ('cane', True), (\"chili's\", True), ('chunk', True), ('comment', True), ('crap', True), ('dirt', True), ('empire', True), ('eww', True), ('fully', True), ('giving', True), ('grow', True), ('jesus', True), ('mcmuffin', True), ('nappy', True), ('putting', True), ('qw', True), ('rained', True), ('risk', True), ('studying', True), ('sunburn', True), ('traffic', True), ('tune', True), ('allowed', True), ('famous', True), ('finding', True), ('gas', True), ('knew', True), ('locked', True), ('mcfly', True), ('sims', True), ('sort', True), ('task', True), ('bug', True), ('copy', True), ('explode', True), ('german', True), ('stuffed', True), ('white', True), ('bout', True), ('anythin', True), ('exist', True), ('freezing', True), ('given', True), ('hav', True), ('hella', True), ('rabbit', True), ('sat', True), ('sean', True), ('sky', True), ('torn', True), ('upset', True), ('hurt', True), ('cleaning', True), ('shit', True), ('stuck', True), ('tired', True), ('sad', True), ('change', True), ('apartment', True), ('awake', True), ('cook', True), ('goin', True), ('math', True), ('started', True), ('front', True), ('fucking', True), ('account', True), ('brightens', True), ('dunno', True), ('ffs', True), ('hungry', True), ('library', True), ('lost', True), ('middle', True), ('pay', True), ('pc', True), ('power', True), ('raining', True), ('rd', True), ('slow', True), ('told', True), ('travelin', True), ('usual', True), ('walking', True), ('weird', True), ('loving', True), ('sexy', True), ('darn', True), ('glass', True), ('plant', True), ('sam', True), ('turned', True), ('wing', True), ('confusing', True), ('fire', True), ('oven', True), ('sending', True), ('ugh', True), ('agree', True), ('bleh', True), ('ca', True), ('dang', True), ('dead', True), ('debating', True), ('dentist', True), ('eaten', True), ('everywhere', True), ('field', True), ('filling', True), ('fixed', True), ('gb', True), ('grr', True), ('http://myloc.me/', True), ('husband', True), ('id', True), ('jus', True), ('kill', True), ('lesson', True), ('longest', True), ('looming', True), ('losing', True), ('lounge', True), ('macadamia', True), ('mess', True), ('pant', True), ('prolly', True), ('recital', True), ('seafood', True), ('server', True), ('soul', True), ('station', True), ('talent', True), ('. . .', True), ('beloved', True), ('box', True), ('britain', True), ('cooky', True), ('deep', True), ('definitely', True), ('fml', True), ('fo', True), ('legal', True), ('managed', True), (\"mom's\", True), ('moro', True), ('neighbour', True), ('self', True), ('stopped', True), ('title', True), ('tool', True), ('van', True), ('cable', True), ('co-worker', True), ('constant', True), ('fifteen', True), ('ii', True), ('mention', True), ('mower', True), ('mri', True), ('munchkins', True), ('pity', True), ('productive', True), ('push', True), ('ross', True), ('singing', True), ('toy', True), ('utah', True), ('worse', True), ('bite', True), ('choose', True), ('fucked', True), ('grumble', True), ('joy', True), ('killer', True), ('panera', True), ('perhaps', True), ('pissing', True), ('plz', True), ('scan', True), ('throw', True), ('uncomfortable', True), ('catch', True), ('chance', True), ('co', True), ('file', True), ('half', True), ('slept', True), ('dvd', True), ('switch', True), ('wonderful', True), ('hard', True), ('sitting', True), ('mr', True), ('poor', True), ('died', True), ('tweeting', True), ('either', True), ('throat', True), ('washed', True), ('damn', True), ('pissed', True), ('sprained', True), ('fun', True), ('mommy', True), ('asshole', True), ('hating', True), ('literally', True), ('love', True), ('happy', True), ('left', True), ('surprise', True), ('thank', True), ('ahhh', True), ('finish', True), (':/', True), ('boo', True), ('brighter', True), ('bus', True), ('channel', True), ('charging', True), ('ditto', True), ('double', True), ('garden', True), ('iphone', True), ('lay', True), ('mall', True), ('missing', True), ('move', True), ('picnic', True), ('possibility', True), ('public', True), ('puter', True), ('quiz', True), ('shift', True), ('shut', True), ('storm', True), ('studyin', True), ('youtube', True), ('waiting', True), ('internet', True), ('bye', True), ('foot', True), ('plus', True), ('yea', True), ('listen', True), ('clean', True), ('high', True), ('laptop', True), ('packing', True), ('saying', True), ('wake', True), ('apparently', True), ('aw', True), ('con', True), ('exciting', True), ('jon', True), ('law', True), ('lie', True), ('load', True), ('lonely', True), ('quiet', True), ('senior', True), ('soo', True), ('spend', True), ('bgt', True), ('airport', True), ('j', True), ('kinda', True), ('hubby', True), ('lake', True), ('sync', True), ('wrist', True), ('broke', True), ('parent', True), ('breakfast', True), ('reading', True), ('dad', True), ('abroad', True), ('e', True), ('else', True), ('exam', True), ('figure', True), ('horrible', True), ('finally', True), ('disappointed', True), ('sucked', True), ('sadly', True), ('inside', True), ('never', True), ('wasted', True), (':-p', True), ('august', True), ('hold', True), ('liverpool', True), ('lookin', True), ('queen', True), ('released', True), ('stood', True), ('theatre', True), ('twitterville', True), ('yummy', True), ('minute', True), ('attending', True), ('character', True), ('chick', True), ('community', True), ('deck', True), ('faith', True), ('goodmorning', True), ('impressed', True), ('marathon', True), ('xxx', True), ('worst', True), ('glad', True), ('lovely', True), ('band', True), ('jk', True), ('cry', True), ('boredom', True), ('ending', True), ('heck', True), ('ily', True), ('katy', True), ('knee', True), ('recipe', True), ('scared', True), ('teeth', True), ('bloody', True), ('degree', True), ('flight', True), ('hello', True), ('oops', True), ('problem', True), ('short', True), ('feelin', True), ('babe', True), ('btw', True), ('dealership', True), ('deb', True), ('different', True), ('express', True), ('http://yfrog.com/', True), ('kiss', True), ('ughh', True), ('sometimes', True), ('aim', True), ('cleaned', True), ('salad', True), ('tennis', True), ('relief', True), ('sick', True), ('fantastic', True), ('relax', True), ('moving', True), ('nite', True), ('talk', True), ('death', True), ('hang', True), ('note', True), ('shoe', True), ('tgif', True), ('called', True), ('nap', True), ('red', True), ('without', True), ('hehe', True), ('gift', True), ('sleepy', True), ('tried', True), (\"what's\", True), ('thanks', True), ('ever', True), ('every', True), ('seriously', True), ('word', True), ('card', True), ('shower', True), ('bos', True), ('evil', True), ('fed', True), ('focus', True), ('forced', True), ('geography', True), ('riding', True), ('ruined', True), ('sister', True), ('loved', True), ('proud', True), ('favorite', True), ('board', True), ('crashed', True), ('microsoft', True), ('texting', True), ('sooo', True), ('part', True), ('set', True), ('warm', True), ('best', True), ('help', True), ('daddy', True), ('etc', True), ('excited', True), ('lolll', True), ('review', True), ('bitch', True), ('drive', True), ('awesome', True), ('surprised', True), ('sigh', True), ('english', True), ('fuckin', True), ('floor', True), ('toe', True), ('asking', True), ('wow', True), ('hated', True), ('rubbish', True), ('mama', True), ('fones', True), ('money', True), ('xx', True), ('break', True), ('lil', True), ('test', True), ('woke', True), ('keep', True), ('super', True), ('pain', True), ('ce', True), ('downloaded', True), ('laugh', True), ('surgery', True), ('there', True), ('run', True), ('air', True), ('cannot', True), ('crazy', True), ('side', True), ('check', True), ('miss', True), ('accomplished', True), ('appreciate', True), ('artist', True), ('yum', True), ('anybody', True), ('dance', True), ('round', True), ('buy', True), ('facebook', True), ('goodbye', True), ('homework', True), ('instead', True), ('lazy', True), ('line', True), ('meeting', True), ('online', True), ('ticket', True), ('wifey', True), ('wonder', True), ('write', True), ('believe', True), ('sorry', True), ('wanted', True), ('adorable', True), ('woohoo', True), ('writing', True), ('mom', True), ('probably', True), ('ah', True), ('mad', True), ('ppl', True), ('dish', True), ('learn', True), ('cried', True), ('hair', True), ('worry', True), ('fast', True), ('jb', True), ('open', True), ('woman', True), ('ya', True), ('omg', True), ('god', True), ('actually', True), ('brother', True), ('old', True), ('fresh', True), ('bank', True), ('c', True), ('class', True), ('far', True), ('helen', True), ('mac', True), ('mind', True), ('slowest', True), ('stay', True), ('story', True), ('train', True), ('momma', True), ('especially', True), ('received', True), ('goodnight', True), ('wait', True), ('nervous', True), ('haha', True), ('dog', True), ('drink', True), ('free', True), ('illegal', True), ('stuff', True), ('wid', True), ('yeah', True), ('yesterday', True), ('guess', True), ('dont', True), ('ipod', True), ('grrr', True), ('tweetdeck', True), ('town', True), ('dropped', True), ('freaking', True), ('drunk', True), ('piss', True), ('special', True), ('hoping', True), ('ship', True), ('surprisingly', True), ('missed', True), ('pm', True), ('till', True), ('win', True), ('mouse', True), ('office', True), ('would', True), ('amazing', True), ('download', True), ('group', True), ('happened', True), ('july', True), ('nose', True), ('pick', True), ('#juddday', True), ('adium', True), ('adult', True), ('angel', True), ('bbl', True), ('bigger', True), ('blanket', True), ('blessed', True), ('blonde', True), ('blown', True), ('boost', True), ('boss', True), ('bum', True), ('bumped', True), ('butterfly', True), ('candy', True), ('cape', True), ('carnival', True), ('catching', True), ('catwalk', True), ('cent', True), ('cheated', True), ('chem', True), ('closer', True), ('conan', True), ('congratulation', True), ('crab', True), ('crimson', True), ('cuddle', True), ('cue', True), ('dane', True), ('darkness', True), ('decision', True), ('delicious', True), ('delivered', True), ('denver', True), ('designed', True), ('detroit', True), ('diet', True), ('discovered', True), ('dollhouse', True), ('downstairs', True), ('dragon', True), ('dubai', True), ('dylan', True), ('earth', True), ('editor', True), ('edmonton', True), ('emotional', True), ('energy', True), ('everrr', True), ('example', True), ('exotic', True), ('exploring', True), ('fightstar', True), ('force', True), ('fudge', True), ('fï', True), ('gahh', True), ('girlies', True), ('gm', True), ('grape', True), ('gravity', True), ('hayes', True), ('healthy', True), ('hiccup', True), ('hid', True), ('hittin', True), ('hoo', True), ('hooked', True), ('houston', True), (\"how's\", True), ('http://tinyurl.com/m', True), ('humid', True), ('inspiration', True), ('interest', True), ('isaac', True), (\"it'll\", True), ('jbs', True), ('jenn', True), ('jess', True), ('joint', True), ('knife', True), ('kno', True), ('ky', True), ('lifting', True), ('logging', True), ('lord', True), ('los', True), ('medicine', True), ('mickey', True), ('minneapolis', True), ('morrow', True), ('msn', True), (\"nan's\", True), ('newborn', True), ('nlc', True), ('nola', True), ('north', True), ('nosebleed', True), ('notch', True), ('nursing', True), ('offering', True), ('orlando', True), ('outfit', True), ('partner', True), ('partyin', True), ('partying', True), ('phil', True), ('playlist', True), ('plenty', True), ('poland', True), ('pork', True), ('prince', True), ('pritchard', True), ('pro', True), ('pushed', True), ('quest', True), ('quit', True), ('rainbow', True), ('rate', True), ('replying', True), ('respect', True), ('returned', True), ('rf', True), ('rockin', True), ('rogers', True), ('roof', True), ('roundtable', True), ('scott', True), ('selling', True), ('shared', True), ('shite', True), ('sleeve', True), ('sober', True), ('speech', True), ('strep', True), ('survive', True), ('swim', True), ('swimsuit', True), ('talked', True), ('te', True), ('thunderstorm', True), ('tiff', True), ('todayy', True), ('tomorow', True), ('tweeties', True), ('twittered', True), ('untill', True), ('verizon', True), ('vet', True), ('waaah', True), ('weed', True), ('workshop', True), ('yahoo', True), ('dress', True), ('hurry', True), ('offer', True), ('random', True), ('shining', True), ('system', True), ('voice', True), ('yall', True), ('starbucks', True), ('cap', True), ('jimmy', True), ('older', True), ('teach', True), ('wed', True), ('bee', True), ('buddy', True), ('lap', True), ('na', True), ('tooo', True), ('#movie', True), ('=d', True), ('anthony', True), (\"anyone's\", True), ('appreciation', True), ('avatar', True), (\"baby's\", True), ('babysitter', True), ('bacon', True), ('boreddd', True), ('callback', True), ('carr', True), ('celebrity', True), ('challenging', True), ('chamber', True), ('chili', True), ('climb', True), ('club', True), ('collection', True), ('commence', True), ('complaining', True), ('corey', True), ('crime', True), ('cure', True), ('development', True), ('diagnosis', True), ('dj', True), ('dressed', True), ('edge', True), ('escape', True), ('expectation', True), ('farewell', True), ('freak', True), ('gd', True), ('gig', True), ('gum', True), ('hehehe', True), ('ho', True), ('ian', True), ('idk', True), ('irc', True), ('jamie', True), ('joined', True), ('journal', True), ('king', True), ('lasagna', True), ('lift', True), ('lonelyyy', True), ('luckily', True), ('motivation', True), ('muffin', True), ('myspace', True), ('naman', True), ('nanny', True), ('nephew', True), ('oklahoma', True), ('operation', True), ('pancake', True), ('papa', True), ('paris', True), ('paypal', True), ('pjs', True), ('pose', True), ('posting', True), ('ram', True), ('rape', True), ('rear', True), ('required', True), ('responding', True), ('retainer', True), ('rose', True), ('saga', True), ('seattle', True), ('space', True), ('speaking', True), ('stack', True), ('step', True), ('stock', True), ('sugar', True), ('supporter', True), ('swag', True), ('temptation', True), ('tesco', True), ('theme', True), ('tim', True), ('tmw', True), ('touching', True), ('twitpic', True), ('ups', True), ('user', True), ('vid', True), ('wayy', True), ('wes', True), ('whale', True), ('wheat', True), ('workin', True), ('wot', True), ('xbox', True), ('yeahh', True), ('war', True), ('tea', True), ('hd', True), ('lmfao', True), ('mini', True), ('yaaay', True), ('everything', True)]\n"
     ]
    }
   ],
   "source": [
    "# training model\n",
    "analyser.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f952def2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happiness with confidence : 0.48\n"
     ]
    }
   ],
   "source": [
    "# predict the sentiment with the model\n",
    "# sent = input(\"Enter a sentence : \")\n",
    "sent = \"This movie is awesome\"\n",
    "sentiment , confidence = analyser.classify(sent)\n",
    "print(sentiment , \"with confidence :\" , confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a48e1ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle of emotion\n",
    "def handleEmotion(user_response,classfier = None):\n",
    "    analyser = EmotionAnalyser.getInstance(\".\")\n",
    "    sentiment , confidence = analyser.classify(user_response,classfier)        \n",
    "    possitive = ['enthusiasm','fun','happiness','love','surprise','relief']\n",
    "    negative = ['anger','boredom','hate','sadness','worry']\n",
    "    nutral = ['empty','neutral'] \n",
    "    if sentiment in nutral:\n",
    "        return \"I am sorry! I don't understand you\" , sentiment\n",
    "    if sentiment in possitive:\n",
    "        return random.choice([\"I am happy for your \"+sentiment,\"Keep up your good feelings :)\",\"Hooray!\",\"Good for you\",\"I am happy for you\"]) , sentiment\n",
    "    if sentiment in negative:\n",
    "        return random.choice([\"You aren't alone\",\"Cheer up\",\"I am sad for your \"+sentiment,\"It's ganna be okay\",\"Sorry to hear that :(\",\"It will be alright\",\"It's bad for you to feel \"+sentiment]) , sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "232f4e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the response from corpus to the user's questions\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def response(user_response):\n",
    "\n",
    "    # add user_response to sent_tokens \n",
    "    sent_tokens.append(user_response) \n",
    "\n",
    "    # Apply TF-IDF\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=normalize)\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "\n",
    "    # Apply cosine similarity to user_response and the corpus\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    print('cosine_similarity:',vals)\n",
    "    ##! argsort(): Returns the indices that would sort an array. \n",
    "    idx=vals.argsort()[0][-2]\n",
    "    print('indices that would sort:',vals.argsort())\n",
    "    ##! flatten(): Return a copy of the array collapsed into one dimension.\n",
    "    flat = vals.flatten()\n",
    "    print('flatten vals:',flat)\n",
    "    flat.sort()\n",
    "    print('flatten vals:',flat)\n",
    "    req_tfidf = flat[-2]\n",
    "    print('potential ans:',req_tfidf)\n",
    "    \n",
    "    if(req_tfidf==0):\n",
    "        return \"I am sorry! I don't understand you\"\n",
    "    else:\n",
    "        result = sent_tokens[idx]\n",
    "        sent_tokens.remove(user_response)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b3fa3fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build up the conversation along with sentiment\n",
    "def generate_reply(user_response,classfier = None):\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            return \"You are welcome..\" , \"relief\"\n",
    "        else:\n",
    "            greeting = greet(user_response)\n",
    "            if(greeting != None):\n",
    "                return greeting , \"happiness\"\n",
    "            else:     \n",
    "                result = response(user_response)                \n",
    "                if(result == \"I am sorry! I don't understand you\"):\n",
    "                    return handleEmotion(user_response,classfier)\n",
    "                else :\n",
    "                    return result , \"neutral\"\n",
    "    else:      \n",
    "        return \"Bye! take care..\" , \"relief\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5f9ef1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I am glad! You are talking to me', 'happiness')"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trial #1\n",
    "generate_reply(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "dcba70df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity: [[0.26714607 0.         0.         0.         0.         0.\n",
      "  0.14981911 0.         0.         0.         0.         0.\n",
      "  0.         0.12421904 0.         0.         0.         0.\n",
      "  0.12930125 0.16540202 0.13385519 0.         0.16009881 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.26160951 0.         0.         0.20286913\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.        ]]\n",
      "indices that would sort: [[24 25 26 27 28 29 30 31 33 34 36 37 38 39 40 41 42 43 44 45 48 46 23 21\n",
      "   1  2  3  4  5  7  8  9 10 11 47 14 15 16 17 12 13 18 20  6 22 19 35 32\n",
      "   0 49]]\n",
      "flatten vals: [0.26714607 0.         0.         0.         0.         0.\n",
      " 0.14981911 0.         0.         0.         0.         0.\n",
      " 0.         0.12421904 0.         0.         0.         0.\n",
      " 0.12930125 0.16540202 0.13385519 0.         0.16009881 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.26160951 0.         0.         0.20286913\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.        ]\n",
      "flatten vals: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.12421904 0.12930125\n",
      " 0.13385519 0.14981911 0.16009881 0.16540202 0.20286913 0.26160951\n",
      " 0.26714607 1.        ]\n",
      "potential ans: 0.2671460748071607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('chatbot\\nchatbot is a computer program or an artificial intelligence which conducts a conversation via auditory or textual methods.such programs are often designed to convincingly simulate how a human would behave as a conversational partner, thereby passing the turing test.',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_reply(\"What is a chatbot?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "259c42b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity: [[0.10477759 0.         0.         0.         0.         0.\n",
      "  0.41611117 0.         0.11559065 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.20619273 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.        ]]\n",
      "indices that would sort: [[24 48 25 26 27 28 29 30 31 45 33 34 35 36 37 38 39 40 41 42 23 43 22 20\n",
      "   1  2  3  4  5 47  7 46  9 10 11 12 13 14 15 16 17 18 19 21 44  0  8 32\n",
      "   6 49]]\n",
      "flatten vals: [0.10477759 0.         0.         0.         0.         0.\n",
      " 0.41611117 0.         0.11559065 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.20619273 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.        ]\n",
      "flatten vals: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.10477759 0.11559065 0.20619273\n",
      " 0.41611117 1.        ]\n",
      "potential ans: 0.4161111677851399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('background of chatbot\\nin 1950, alan turing\\'s famous article \"computing machinery and intelligence\" was published, which proposed what is now called the turing test as a criterion of intelligence.',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_reply(\"Who is Alan Turing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c6ece4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity: [[0.         0.         0.         0.         0.12865055 0.07629441\n",
      "  0.         0.         0.         0.10790396 0.         0.13833064\n",
      "  0.         0.         0.         0.13064872 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.61682857 0.21683952\n",
      "  0.         0.24719281 0.06889775 0.18161912 0.         0.\n",
      "  0.         0.07619906 0.         0.09648112 0.16672178 0.\n",
      "  0.         1.        ]]\n",
      "indices that would sort: [[ 0 23 48 25 26 27 28 29 30 31 32 33 36 40 41 42 44 47 22 21 24 19  3  6\n",
      "  20  1  2  8 10  7 12 13 14 16 17 18 38 43  5 45  9  4 15 11 46 39 35 37\n",
      "  34 49]]\n",
      "flatten vals: [0.         0.         0.         0.         0.12865055 0.07629441\n",
      " 0.         0.         0.         0.10790396 0.         0.13833064\n",
      " 0.         0.         0.         0.13064872 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.61682857 0.21683952\n",
      " 0.         0.24719281 0.06889775 0.18161912 0.         0.\n",
      " 0.         0.07619906 0.         0.09648112 0.16672178 0.\n",
      " 0.         1.        ]\n",
      "flatten vals: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.06889775 0.07619906 0.07629441 0.09648112 0.10790396 0.12865055\n",
      " 0.13064872 0.13833064 0.16672178 0.18161912 0.21683952 0.24719281\n",
      " 0.61682857 1.        ]\n",
      "potential ans: 0.6168285696582634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('virtual assistant\\nintelligent virtual assistant (iva) or intelligent personal assistant (ipa) is a software agent that can perform tasks or services for an individual based on verbal commands[citation needed].',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_reply(\"What is intelligent virtual assistant?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "6e53d0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 1.]]\n",
      "indices that would sort: [[ 0 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 27 26\n",
      "  25 12  3  4  5  6  7  8  9 10 11 24 13 14 15 16 17 18 19 20 21 22 23  2\n",
      "   1 49 50]]\n",
      "flatten vals: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1.]\n",
      "flatten vals: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1.]\n",
      "potential ans: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('i am bored', 'neutral')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_reply(\"I am bored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3ceb7e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1.]]\n",
      "indices that would sort: [[ 0 27 28 29 30 31 32 33 34 35 36 26 37 39 40 41 42 43 44 45 46 47 48 38\n",
      "  49 25 23  1  2  3  4  5  6  7  8  9 10 24 11 13 14 15 16 17 18 19 20 21\n",
      "  22 12 50]]\n",
      "flatten vals: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1.]\n",
      "flatten vals: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1.]\n",
      "potential ans: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Hooray!', 'happiness')"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_reply(\"I am watching this movie tomorrow night\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4ecebe8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bye\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Bye! take care..', 'relief')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take user query through input()\n",
    "query = input()\n",
    "generate_reply(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ed51e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d944b769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "handsonnlp",
   "language": "python",
   "name": "handsonnlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
